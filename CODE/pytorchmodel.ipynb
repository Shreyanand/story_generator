{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STORY CLOZE GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "n = 10\n",
    "csv_file = \"/Users/shrey/Text Generation/story_cloze/data/process_data.csv\"\n",
    "numpy_file = \"vectors_\"+ str(\"preprocessed\") + \".npy\"\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicGRU(nn.Module):\n",
    "    def __init__(self, hidden_size, n_layers=1, dropout=0.3):\n",
    "        super(BasicGRU, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        #self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "        \n",
    "        self.lin = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "#         # Convert word indexes to embeddings\n",
    "#         embedded = self.embedding(input_seq)\n",
    "#         # Pack padded batch of sequences for RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(input_seq, input_lengths, batch_first=True)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs , batch_first=True)\n",
    "        #print(outputs.shape)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        \n",
    "        output = self.lin(outputs)\n",
    "        #print(outputs.shape)\n",
    "        # Return output and final hidden state\n",
    "        #assert torch.equal(outputs[-1,:,:], hidden.squeeze(0))\n",
    "        #print(hidden.shape, outputs[:,-1,:].shape)\n",
    "        return output\n",
    "    \n",
    "model = BasicGRU(hidden_size = 4800)\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_0 = []\n",
    "story_1 = []\n",
    "class StoryVectors(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, numpy_file, csv_file):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        #\"/Users/shrey/Text Generation/story_cloze/data/stories.csv\"\n",
    "        # \"vectors_\"+ str(n) + \".npy\"\n",
    "        n = 10\n",
    "        self.sentences = pd.read_csv(csv_file).values[:n,1:].reshape(-1).tolist()\n",
    "        vecn = np.load(numpy_file)\n",
    "        vec = vecn.tolist()\n",
    "        \n",
    "        v1, v2, v3, v4, v5 = vec[::5], vec[1::5], vec[2::5], vec[3::5], vec[4::5]\n",
    "        story_0.append([v1[0], v2[0], v3[0], v4[0], v5[0]])\n",
    "        story_1.append([v1[1], v2[1], v3[1], v4[1], v5[1]])\n",
    "\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        m = len(v1)\n",
    "        for i in range(m):\n",
    "            self.X.append([v1[i], v2[i], v3[i], v4[i]])\n",
    "            self.y.append([v2[i], v3[i], v4[i], v5[i]])\n",
    "#             #lengths.append(4)\n",
    "#             self.X.append([v1[i], v2[i], v3[i]])\n",
    "#             self.y.append(v4[i])\n",
    "#             #lengths.append(3)\n",
    "#             self.X.append([v1[i], v2[i]])\n",
    "#             self.y.append(v3[i])\n",
    "#             #lengths.append(2)\n",
    "#             self.X.append([v1[i]])\n",
    "#             self.y.append(v2[i])\n",
    "#             #lengths.append(1)\n",
    "\n",
    "        assert (len(self.X) == len(self.y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        return [self.X[idx], len(self.X[idx]), self.y[idx]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    lengths = []\n",
    "\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0, 4-datum[1]), (0,0)),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        X.append(padded_vec)\n",
    "        y.append(datum[2])\n",
    "        lengths.append(datum[1])\n",
    "        \n",
    "    ind_dec_order = np.argsort(lengths)[::-1]\n",
    "    X = np.array(X)[ind_dec_order]\n",
    "    lengths = np.array(lengths)[ind_dec_order]\n",
    "    y = np.array(y)[ind_dec_order]\n",
    "    return [torch.FloatTensor(X), \n",
    "            torch.LongTensor(lengths), torch.FloatTensor(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = StoryVectors(numpy_file, csv_file)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1, l1, la1 = iter(train_loader).next()\n",
    "la1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(num_epochs):\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        t0 = time.time()\n",
    "        for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            y_pred = model(data, lengths)\n",
    "            print(y_pred.shape)\n",
    "            loss = criterion(y_pred, labels)\n",
    "            #print(epoch, i,  loss.item())\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 1 == 0:\n",
    "                loss_data = loss.data[0]\n",
    "                #train_losses.append(loss_data)\n",
    "                print(\n",
    "                    'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.\n",
    "                    format(epoch, i * len(data), len(train_loader.dataset),\n",
    "                           100. * i / len(train_loader), loss_data))\n",
    "                \n",
    "        print('Time taken by the epoch: {} seconds'.format(time.time() - t0))\n",
    "\n",
    "            # validate every 100 iterations\n",
    "    #         if i > 0 and i % 100 == 0:\n",
    "    #             # validate\n",
    "    #             val_acc = test_model(val_loader, model)\n",
    "    #             print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "    #                        epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "training(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(csv_file)\n",
    "model = torch.load(\"model/GRU_Ln_np_100.tar\", map_location={'cuda:0': 'cpu'})\n",
    "\n",
    "array = df.values[:,3:].reshape(-1).tolist()\n",
    "vecn = np.load(\"vectors_\"+ str(\"preprocessed\") + \".npy\")\n",
    "vec = vecn.tolist()\n",
    "print(len(vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = StoryVectors(numpy_file, csv_file)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4)\n",
    "\n",
    "def test_model(test_loader):\n",
    "    model.eval()\n",
    "    ranks1, ranks2, ranks3, ranks4 = [], [], [], []\n",
    "    for data, lengths, labels in train_loader:\n",
    "        pred = model(data, lengths)\n",
    "        #print(len(pred.tolist()), len(labels.tolist()))\n",
    "        s1s = data[:,0,:].tolist()\n",
    "        batch_ranks1, batch_ranks2, batch_ranks3, batch_ranks4 = analyse(s1s, pred.tolist(), vecn, labels.tolist(), array)\n",
    "        ranks1.extend(batch_ranks1)\n",
    "        ranks2.extend(batch_ranks2)\n",
    "        ranks3.extend(batch_ranks3)\n",
    "        ranks4.extend(batch_ranks4)\n",
    "\n",
    "    return ranks1, ranks2, ranks3, ranks4\n",
    "\n",
    "\n",
    "ranks1, ranks2, ranks3, ranks4 = test_model(train_loader)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "# from matplotlib import interactive\n",
    "# interactive(True)\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def distr(array):\n",
    "    plt.scatter([i for i in range(len(array))], sorted(array))\n",
    "    plt.savefig('1.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "def freq(array):\n",
    "    plt.hist(array, bins=np.arange(min(array), max(array)+1))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def nn(qvec, vectors, array, k=5):\n",
    "    qvec /= np.linalg.norm(qvec)\n",
    "    vectors = np.asarray([ i / np.linalg.norm(i) for i in vectors.tolist()])\n",
    "    scores = np.dot(qvec, vectors.T).flatten()\n",
    "    #distr(scores)\n",
    "    #analyse(scores)\n",
    "    sorted_args = np.argsort(scores)[::-1]\n",
    "    sentences = [(array[a], scores[a]) for a in sorted_args[:k]]\n",
    "    for i, s in enumerate(sentences):\n",
    "        print (s, sorted_args[i])\n",
    "\n",
    "def analyse(s1s, predicted, vectors, actual, array):\n",
    "    \n",
    "    assert len(predicted) == len(actual), \"Oh\"\n",
    "\n",
    "    vectors = np.asarray([ i / np.linalg.norm(i) for i in vectors.tolist()])\n",
    "    ranks1, ranks2, ranks3, ranks4 = [], [], [], []\n",
    "    count = 0\n",
    "    \n",
    "    for pred, act, s1 in zip(predicted, actual, s1s): #iterating through the batch 0-31\n",
    "        #print(Story)\n",
    "        story = [s1] + act\n",
    "        print(len(story))\n",
    "        [nn(sen, vectors, array, k=1) for sen in story]\n",
    "        \n",
    "        for i, (p,a) in enumerate(zip(pred, act)): #iterating through the sequence 0-3\n",
    "            #print(len(p))\n",
    "            p /= np.linalg.norm(p)\n",
    "            a /= np.linalg.norm(a)\n",
    "            scores = np.dot(p, vectors.T).flatten()\n",
    "            score_actpred = np.dot(p, a)\n",
    "            #print(\"score of act and pred:\", score_actpred)\n",
    "\n",
    "            rank = -1\n",
    "            sorted_scores = sorted(scores, reverse=True)\n",
    "            for index, score in enumerate(sorted_scores):\n",
    "                if np.isclose(score, score_actpred):\n",
    "                    rank = index\n",
    "                    break\n",
    "                    \n",
    "            if i == 0:\n",
    "              ranks1.append(rank)\n",
    "            elif i == 1:\n",
    "              ranks2.append(rank)\n",
    "            elif i == 2:\n",
    "              ranks3.append(rank)\n",
    "              show_inp_out(a,p)\n",
    "            elif i == 3:\n",
    "              ranks4.append(rank)\n",
    "\n",
    "    return ranks1, ranks2, ranks3, ranks4\n",
    "  \n",
    "def show_inp_out(actual, predicted):\n",
    "#     print(\"Input Sentences\")\n",
    "#     inp3 = data[index,3,:]\n",
    "#     inp2 = data[index,2,:]\n",
    "#     inp1 = data[index,1,:]\n",
    "#     inp0 = data[index,0,:]\n",
    "#     nn(inp0.numpy().squeeze().tolist(), vecn, array, k=1)\n",
    "#     nn(inp1.numpy().squeeze().tolist(), vecn, array, k=1)\n",
    "#     nn(inp2.numpy().squeeze().tolist(), vecn, array, k=1)\n",
    "#     nn(inp3.numpy().squeeze().tolist(), vecn, array, k=1)\n",
    "\n",
    "    print(\"Actual Output\")\n",
    "    nn(actual.squeeze().tolist(), vecn, array, k=1)\n",
    "\n",
    "    print(\"Predicted Output\")\n",
    "    nn(predicted.squeeze().tolist(), vecn, array, k=5)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8380942"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = d1.numpy()[p:q,3,:].squeeze().tolist()\n",
    "np.linalg.norm(act-pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello\\n'\n"
     ]
    }
   ],
   "source": [
    "# import subprocess\n",
    "# #subprocess.run([\"python\", \"import sys; print sys.version_info[0]\"], stdout=subprocess.PIPE)\n",
    "# subprocess.check_call([\"python\", \"\"])\n",
    "\n",
    "import subprocess\n",
    "\n",
    "#p = subprocess.check_output([\"python\", \"/Users/shrey/test.py\"])\n",
    "#p = subprocess.check_output([\"echo\" \"hi\"])\n",
    "#p = subprocess.run([\"echo $(python /Users/shrey/test.py)\"], shell=True, stdout=subprocess.PIPE)\n",
    "p1 = subprocess.run(['/usr/bin/python', '/Users/shrey/test.py'], stdin=None, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "print (p1.stdout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Pack Padded and Pad packed #########################\n",
    "# Padd all your sequences with 0 so that they are the same length. \n",
    "# But, record the actual length (unpadded) of each sequence in lengths(int) vector\n",
    "# to the function pack_padded_sequence pass Batch_Size * Longest Sequence length * num_directions(other dimensions)\n",
    "# Also, int tensor length with len(length) = Batch_Size\n",
    "\n",
    "\n",
    "################ Rough Work #####################################\n",
    "\n",
    "# pack = torch.nn.utils.rnn.pack_padded_sequence(X_t, lengths , batch_first=True)\n",
    "# unpack, s = torch.nn.utils.rnn.pad_packed_sequence(pack, batch_first=True)\n",
    "\n",
    "# a = [torch.tensor([3,4,5,6]), torch.tensor([1,2,3]), torch.tensor([3,4])]\n",
    "# print(a)\n",
    "# b = torch.nn.utils.rnn.pad_sequence(a, batch_first=True)\n",
    "# print(b)\n",
    "# c = torch.nn.utils.rnn.pack_padded_sequence(b, batch_first=True, lengths=[4,3,2])\n",
    "# print(c)\n",
    "# d, _ = torch.nn.utils.rnn.pad_packed_sequence(c)\n",
    "# print(d)\n",
    "\n",
    "# batch_size = 3\n",
    "# max_length = 3\n",
    "# hidden_size = 2\n",
    "# n_layers =1\n",
    "\n",
    "# # container\n",
    "# batch_in = torch.zeros((batch_size, 1, max_length))\n",
    "\n",
    "# #data\n",
    "# vec_1 = torch.FloatTensor([[1, 2, 3]])\n",
    "# vec_2 = torch.FloatTensor([[1, 2, 0]])\n",
    "# vec_3 = torch.FloatTensor([[1, 0, 0]])\n",
    "\n",
    "# batch_in[0] = vec_1\n",
    "# batch_in[1] = vec_2\n",
    "# batch_in[2] = vec_3\n",
    "\n",
    "# batch_in = Variable(batch_in)\n",
    "\n",
    "# seq_lengths = [3,2,1] # list of integers holding information about the batch size at each sequence step\n",
    "\n",
    "# # pack it\n",
    "# # pack = torch.nn.utils.rnn.pack_padded_sequence(batch_in, seq_lengths, batch_first=True)\n",
    "# # unpack, _ = torch.nn.utils.rnn.pad_packed_sequence(pack)\n",
    "\n",
    "\n",
    "\n",
    "# steps = []\n",
    "# batch_sizes = []\n",
    "# X_t = X_t.transpose(0, 1)\n",
    "\n",
    "# # # lengths is a Tensor, so we must convert to [int] before reversed()\n",
    "# # lengths_iter = reversed(lengths.tolist())\n",
    "\n",
    "# # batch_size = X_t.size(1)\n",
    "\n",
    "# # if len(lengths) != batch_size:\n",
    "# #     raise ValueError(\"Expected `len(lengths)` to be equal to batch_size, but got \"\n",
    "# #                      \"{} (batch_size={}).\".format(len(lengths), batch_size))\n",
    "\n",
    "# # prev_l = 0\n",
    "# # for i, l in enumerate(lengths_iter):\n",
    "# #     if l > prev_l:\n",
    "# #         c_batch_size = batch_size - i\n",
    "# #         print(X_t[prev_l:l, :c_batch_size])\n",
    "# #         steps.append(X_t[prev_l:l, :c_batch_size].contiguous().view(-1, *X_t.size()[2:]))\n",
    "# #         batch_sizes.extend([c_batch_size] * (l - prev_l))\n",
    "# #         prev_l = l\n",
    "\n",
    "# #     elif prev_l > l:\n",
    "# #         raise ValueError(\"'lengths' array has to be sorted in decreasing order\")\n",
    "\n",
    "# padded_vec = np.pad(np.array([v1[0]]),\n",
    "#                                 pad_width=((3,0), (0,0)),\n",
    "#                                 mode=\"constant\", constant_values=0)\n",
    "# print(padded_vec[1,:])\n",
    "# len(v1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv(\"/Users/shrey/Text Generation/story_cloze/data/stories.csv\")\n",
    "\n",
    "# import numpy as np\n",
    "# n = 10 \n",
    "# array = df.values[:n,1:].reshape(-1).tolist()\n",
    "# vecn = np.load(\"vectors_\"+ str(n) + \".npy\")\n",
    "# vec = vecn.tolist()\n",
    "# len(vec)\n",
    "\n",
    "# def nn(qvec, vectors, array, k=5):\n",
    "#     #qvec /= norm(qvec)\n",
    "#     scores = np.dot(qvec, vectors.T).flatten()\n",
    "#     sorted_args = np.argsort(scores)[::-1]\n",
    "#     sentences = [array[a] for a in sorted_args[:k]]\n",
    "#     for i, s in enumerate(sentences):\n",
    "#         print (s, sorted_args[i])\n",
    "        \n",
    "# vt = vec[::6]\n",
    "# v1 = vec[1::6]\n",
    "# v2 = vec[2::6]\n",
    "# v3 = vec[3::6]\n",
    "# v4 = vec[4::6]\n",
    "# v5 = vec[5::6]\n",
    "\n",
    "# v0 = np.zeros(4800).tolist()\n",
    "# X = []\n",
    "# y = []\n",
    "# lengths = []\n",
    "# m = len(v1)\n",
    "# for i in range(m):\n",
    "#     X.append([v1[i], v2[i], v3[i], v4[i]])\n",
    "#     y.append(v5[i])\n",
    "#     lengths.append(4)\n",
    "#     X.append([v0, v1[i], v2[i], v3[i]])\n",
    "#     y.append(v4[i])\n",
    "#     lengths.append(3)\n",
    "#     X.append([v0, v0, v1[i], v2[i]])\n",
    "#     y.append(v3[i])\n",
    "#     lengths.append(2)\n",
    "#     X.append([v0 , v0 , v0 , v1[i]])\n",
    "#     y.append(v2[i])\n",
    "#     lengths.append(1)\n",
    "\n",
    "    \n",
    "# data = [ (k, m, l)  for k, m, l in sorted(zip(X,y,lengths), key=lambda pair: pair[2], reverse=True)]\n",
    "# X = list(zip(*data))[0]\n",
    "# y = list(zip(*data))[1]\n",
    "# lengths = list(zip(*data))[2]\n",
    "    \n",
    "# X = np.asarray(X) #.reshape(5, 8, 4, 4800) # X.shape is (samples, timesteps, features)\n",
    "# y = np.asarray(y) #.reshape(5, 8, 4800)\n",
    "# lengths = np.asarray(lengths) #.reshape(5, 8)\n",
    "\n",
    "# from torch.autograd import Variable\n",
    "# X_t =torch.FloatTensor(X)\n",
    "# y_t =torch.FloatTensor(y)\n",
    "# lengths = torch.IntTensor(lengths)\n",
    "# print(X_t.requires_grad)\n",
    "\n",
    "\n",
    "########################## Py torch Basics ###################\n",
    "# import torch\n",
    "# x = torch.ones(2, 2, requires_grad=True)\n",
    "# print(x)\n",
    "\n",
    "# y = x * 2\n",
    "# z = y + 5\n",
    "# out = z * z / 2\n",
    "# out = out.mean()\n",
    "\n",
    "# out.grad\n",
    "\n",
    "\n",
    "############ Laoding Validation ##############################\n",
    "\n",
    "# val_dataset = VocabDataset(val_data, char2id)\n",
    "# val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "#                                            batch_size=BATCH_SIZE,\n",
    "#                                            collate_fn=vocab_collate_func,\n",
    "#                                            shuffle=True)\n",
    "\n",
    "# test_dataset = VocabDataset(test_data, char2id)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "#                                            batch_size=BATCH_SIZE,\n",
    "#                                            collate_fn=vocab_collate_func,\n",
    "#                                            shuffle=False)\n",
    "\n",
    "\n",
    "# array = df.values[:,3:].reshape(-1).tolist()\n",
    "# vecn = np.load(\"vectors_\"+ str(\"preprocessed\") + \".npy\")\n",
    "# vec = vecn.tolist()\n",
    "# print(len(vec))\n",
    "\n",
    "# def nn(qvec, vectors, array, k=5):\n",
    "#     qvec /= np.linalg.norm(qvec)\n",
    "#     vectors = np.asarray([ i / np.linalg.norm(i) for i in vectors.tolist()])\n",
    "#     scores = np.dot(qvec, vectors.T).flatten()\n",
    "#     #distr(scores)\n",
    "#     #analyse(scores)\n",
    "#     sorted_args = np.argsort(scores)[::-1]\n",
    "#     sentences = [(array[a], scores[a]) for a in sorted_args[:k]]\n",
    "#     for i, s in enumerate(sentences):\n",
    "#         print (s, sorted_args[i])\n",
    "        \n",
    "# #len(story_0[0][0])        \n",
    "# # story_0s = [nn(i, vecn, array, k=1) for i in story_0[0]]\n",
    "# # story_1s = [nn(i, vecn, array, k=1) for i in story_1[0]]\n",
    "# # print(story_1s)\n",
    "\n",
    "\n",
    "######################## Old Code ################################################\n",
    "\n",
    "# train_dataset = StoryVectors(numpy_file, csv_file)\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "#                                            batch_size=BATCH_SIZE,\n",
    "#                                            collate_fn=vocab_collate_func,\n",
    "#                                            shuffle=True,\n",
    "#                                            num_workers=4)\n",
    "\n",
    "# d1, l1, la1 = iter(train_loader).next()\n",
    "\n",
    "# d1[1,3,:].shape\n",
    "# la1[1,:].shape\n",
    "\n",
    "# p = 1\n",
    "# q = 2\n",
    "# pred = model(d1[p:q,:,:], torch.tensor([1]))\n",
    "# pred = pred.squeeze().detach().numpy()\n",
    "\n",
    "# # nn(vec[7], vecn, array)\n",
    "# # array[7]\n",
    "\n",
    "# # nn(vec[7], vecn, array)\n",
    "# # array[7]\n",
    "# print(\"Input Sentence\")\n",
    "# nn(d1.numpy()[p:q,3,:].squeeze().tolist(), vecn, array, k=1)\n",
    "\n",
    "\n",
    "# print(\"Actual Output\")\n",
    "# nn(la1.numpy()[p:q,:].squeeze().tolist(), vecn, array, k=1)\n",
    "\n",
    "# print(\"Predicted Output\")\n",
    "# nn(pred.tolist(), vecn, array, k=20)\n",
    "\n",
    "# act = la1.numpy()[p:q,:].squeeze().T\n",
    "# #analyse(pred, vecn, act)\n",
    "# # norm_pred  = pred / np.linalg.norm(pred)\n",
    "\n",
    "# # norm_act =  act / np.linalg.norm(act)\n",
    "# np.dot(pred, act)\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# m = nn.Linear(20, 30)\n",
    "# inp = torch.randn(128, 20)\n",
    "# output = m(inp)\n",
    "# print(output.size())\n",
    "# print(inp.size())\n",
    "\n",
    "\n",
    "# # pred_ = np.load('pred.npy')\n",
    "\n",
    "# # np.dot(pred, pred_.T)\n",
    "# #print(pred_.shape)\n",
    "# nn(pred_, dataset2[2,:,:], sentences2[2,:], k=5)\n",
    "# pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectors = np.load(\"data/\"+ experiment + \"/test_vectors.npy\")\n",
    "test_sentences = np.load(\"data/\"+ experiment + \"/test_sentences.npy\")\n",
    "train_vectors = np.load(\"data/\"+ experiment + \"/train_vectors.npy\")\n",
    "train_sentences = np.load(\"data/\"+ experiment + \"/train_sentences.npy\")\n",
    "\n",
    "vectors = np.concatenate((train_vectors, test_vectors), axis=0)\n",
    "sentences = np.concatenate((test_sentences, train_sentences), axis=0)\n",
    "\n",
    "def pad(vector, length):\n",
    "    padded_vec = np.pad(vector,\n",
    "                        pad_width=((0, 4-length), (0,0)),\n",
    "                        mode=\"constant\", constant_values=0)\n",
    "    return padded_vec\n",
    "\n",
    "def nn(qvec, vectors, array, k=5):\n",
    "#     print(\"processing\")\n",
    "#     qvec /= np.linalg.norm(qvec)\n",
    "#     vectors = np.asarray([ i / np.linalg.norm(i) for i in vectors.tolist()])\n",
    "    print(\"computing scores\")\n",
    "    scores = np.dot(qvec, vectors.T).flatten()\n",
    "    #distr(scores)\n",
    "    #analyse(scores)\n",
    "    print(\"sorting scores\")\n",
    "    sorted_args = np.argsort(scores)[::-1]\n",
    "    sentences = [(array[a], scores[a]) for a in sorted_args[:k]]\n",
    "    for i, s in enumerate(sentences):\n",
    "        print (s, sorted_args[i])\n",
    "        \n",
    "        \n",
    "def suggestions(vector, length, vectors, sentences, k=5):\n",
    "    pv1 = pad(vector, length)\n",
    "    print(\"predicting vector\")\n",
    "    pred = model(torch.FloatTensor([pv1]), torch.LongTensor([4]))\n",
    "    pred = pred.detach().numpy().squeeze(axis=0)\n",
    "    pred = pred[-1]\n",
    "    print(\"searching sentence\")\n",
    "    nn(pred, vectors, sentences, k=5)\n",
    " \n",
    "\n",
    "v1 = np.asarray([train_vectors[0]])\n",
    "v2 = np.asarray(train_vectors[0:2])\n",
    "v3 = np.asarray(train_vectors[0:3])\n",
    "v4 = np.asarray(train_vectors[0:4])\n",
    "\n",
    "l1, l2, l3, l4 = 1, 2, 3, 4\n",
    "\n",
    "suggestions(v4, l4, train_vectors, train_sentences, k=5)\n",
    "print(train_sentences[0], train_sentences[1], train_sentences[2], train_sentences[3], train_sentences[4])\n",
    "# suggestions(v2, l2, vectors, sentences, k=5)\n",
    "# suggestions(v3, l3, vectors, sentences, k=5)\n",
    "# suggestions(v4, l4, vectors, sentences, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_vectors = np.load(\"data/\"+ experiment + \"/test_vectors.npy\")[:5]\n",
    "# test_sentences = np.load(\"data/\"+ experiment + \"/test_sentences.npy\")[:5]\n",
    "\n",
    "# test_vectors, temp = test_vectors[:1], np.expand_dims(test_vectors[4], axis=0)\n",
    "\n",
    "\n",
    "# test_vectors = np.pad(test_vectors, pad_width=((0, 3), (0,0)), mode=\"constant\", constant_values=0)\n",
    "\n",
    "# print(test_vectors.shape, temp.shape)\n",
    "\n",
    "# test_vectors = np.concatenate((test_vectors, temp), axis=0)\n",
    "\n",
    "# print(test_vectors.shape)\n",
    "\n",
    "\n",
    "\n",
    "#nn(pred, d, s, k=5)\n",
    "\n",
    "# test_vectors = np.load(\"data/\"+ experiment + \"/test_vectors.npy\")\n",
    "# test_sentences = np.load(\"data/\"+ experiment + \"/test_sentences.npy\")\n",
    "\n",
    "# for data, lengths, labels, sentences in test_loader:\n",
    "#     pred = model(data, lengths)\n",
    "#     pred = pred.detach().numpy().squeeze(axis=0)\n",
    "#     pred = pred[-1]\n",
    "#     print(\"searching sentence\")\n",
    "#     nn(pred, test_vectors, test_sentences, k=5)\n",
    "    \n",
    "# test_sentences[:5]  \n",
    "\n",
    "# test_dataset = StoryVectors(dataset4[:, 6:7, :], sentences4[:, 6:7])\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "#                                            batch_size=1,\n",
    "#                                            collate_fn=vocab_collate_func,\n",
    "#                                            shuffle=True,\n",
    "#                                            num_workers=4)\n",
    "\n",
    "# model4.eval()\n",
    "\n",
    "# for data, lengths, labels, sentences in test_loader:\n",
    "#         pred = model4(data, lengths)\n",
    "\n",
    "        \n",
    "# pred = pred.detach().numpy().squeeze()\n",
    "# print(pred.shape)\n",
    "# print(sentences)\n",
    "# no, sample, dim = dataset4.shape\n",
    "# d = dataset4.reshape(no*sample, dim)\n",
    "# s = sentences4.reshape(no*sample)\n",
    "# print(d.shape)\n",
    "# nn(pred, d, s, k=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
